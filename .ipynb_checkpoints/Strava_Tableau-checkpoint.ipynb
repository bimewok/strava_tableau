{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/bimewok/strava_tableau/blob/main/Strava_Tableau.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WL9eqDPevJfR"
   },
   "source": [
    "# Creating a Tableau Dashboard of My Strava Activities\n",
    "Author: Ben Garrett<br>\n",
    "Date created: 2021-03-01<br>\n",
    "Last modified: 2021-03-01<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQhyANOJOp9v"
   },
   "source": [
    "# Introduction\n",
    "The goal of this project is to extract tabular and spatial data from my Strava account and create a meaningful map and dashboard using python and Tableau.\n",
    "Strava is a social network for athletes to share their physical activities with their friends and local communities. I have been using Strava for a few years and have hundreds of hikes, runs and bike rides logged on the site. Although Strava makes it easy to see a list of your activities on their app, they provide few useful maps and charts to users. Luckily, I can fix that!<br><br> \n",
    "www.strava.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smFVp49ewLhB"
   },
   "source": [
    "## About the Data\n",
    "\n",
    "The data originated from any number of apps or smartwatches. I use a Suunto GPS watch to record each activity and the watch's app sends data to Strava when I sync it to my phone. The information of interest is average speed, elevation gain, distance, duration, etc. On Strava, a typical activity upload looks like the following:\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"https://github.com/bimewok/strava_tableau/blob/main/strava_example_upload.PNG?raw=true\" width=\"942\" height=\"867\" align=\"center\"/>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Fortunately, Strava also allows you to bulk download your spatial and tabular data. The data comes in 40+ csv's and the spatial data comes as a single .fit  file for each activity. The end goal is a single table with all of the important tabular data and spatial data all in one place, not hundreds of files in different formats. While you can use open source tools to convert .fit files to a more typical spatial format like `.gpx` or `.shp`, I decided to tackle the problem with python in a more interesting way...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1V3Lv4nn2dub"
   },
   "source": [
    "## Getting Activity Spatial Data With Python\n",
    "\n",
    "On Strava's webpage for each activity, there is a link to download the activity track as a easy-to-use `.gpx` file. Conveniently, the url for each activity page is predictable and reproducible if one know the activity name they want to receive. You can use Strava's API to routinely get these files with authentication, but I found that process no simpler or quicker than making a simple request. Thus, I decided to use python to direct my browser (already logged in to my account) to download each file via the link on the activity webpage. \n",
    "<br>\n",
    "\n",
    "<img src=\"https://github.com/bimewok/strava_tableau/blob/main/Inkeddownload%20gpx_LI.jpg?raw=true\" width=\"301\" height=\"371\" align=\"center\"/>\n",
    "<br>\n",
    "<br>\n",
    "Using the `activities.csv` downloaded from the bulk download, it is easy to generate a url to make the request to get each file. Unfortunately, each file is named as the non-unique activity name, so the tricky part of the following code is finding the actively downloading file and logging its name. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSTcZFtXDLHD"
   },
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "import os.path\n",
    "\n",
    "save_path = r'C:\\garrett_workspace\\tableau\\strava_dashboard\\gpx'\n",
    "chrome_path = r'C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe'\n",
    "webbrowser.register('chrome', None,webbrowser.BackgroundBrowser(chrome_path))\n",
    "\n",
    "activity_data = pd.read_csv(r'C:\\garrett_workspace\\Strava_Backup_Data\\activities.csv')\n",
    "activity_data['gpx_filepath'] = np.nan\n",
    "\n",
    "full_file_names = []\n",
    "\n",
    "for i in range(len(activity_data)):\n",
    "    if activity_data['Filename'][i] != np.nan:\n",
    "        activity_id = int(activity_data['Activity ID'][i])\n",
    "        url = 'https://www.strava.com/activities/{}/export_gpx'.format(activity_id)\n",
    "        webbrowser.get('chrome').open_new_tab(url)\n",
    "        time.sleep(15)\n",
    "        '''the following loop checks to see if the file has finished downloading\n",
    "        to ensure only one file gets downloaded at a time.'''\n",
    "        for z in range(1, 30000):\n",
    "            files = listdir(save_path)\n",
    "            for x in files:\n",
    "                if 'Unconfirmed' in x:\n",
    "                    time.sleep(1)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "        for file in files:\n",
    "            full_file_names.append(save_path+'\\{}'.format(file))\n",
    "        latest_file = max(full_file_names, key=os.path.getctime)\n",
    "        activity_data.loc[i, 'gpx_filepath'] = str(latest_file)\n",
    "        \n",
    "activity_data.to_csv(r'C:\\garrett_workspace\\Strava_Backup_Data\\activities.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Uw65BdiFCzt"
   },
   "source": [
    "## Combining Data Into One Format\n",
    "\n",
    "Now that I have all of the tabluar activity data and their associated .gpx files, the next step is to combine them into one format where all attributes can live. The best file format for this is the shapefile, which contains a database table and spatial information in a format that all GIS software and many database tools can read (PostgreSQL via PostGIS, Tableau, many others). To convert the files, the python library `gpxpy` can parse the gpx files and `geopandas` (includes `shapely`) can write the shapefile. The general workflow is this:<br>\n",
    "- open each gpx\n",
    "- pull out each point (coordinate pair of vertex on line), add all points to a shaply linestring object, append object into geopandas geodataframe.\n",
    "- save shapefile via geopandas (and manually write a .prj file containing spatial projection information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMZPbARbH0Wd"
   },
   "outputs": [],
   "source": [
    "import gpxpy\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString\n",
    "\n",
    "tabular = pd.read_csv(r'C:\\garrett_workspace\\tableau\\strava_dashboard\\activities.csv')\n",
    "\n",
    "geo_dataframe = gpd.GeoDataFrame(tabular)\n",
    "geo_dataframe['geometry'] = None\n",
    "\n",
    "for index in range(len(geo_dataframe)):\n",
    "    filepath = geo_dataframe['gpx_filepath'][index]\n",
    "    file = open(filepath, 'r')\n",
    "    gpx = gpxpy.parse(file)\n",
    "    \n",
    "    points = []\n",
    "    \n",
    "    for track in gpx.tracks:\n",
    "        for segment in track.segments:\n",
    "            for point in segment.points:\n",
    "                points.append(tuple([point.longitude, point.latitude]))\n",
    "                \n",
    "    line = LineString(points)\n",
    "    geo_dataframe.loc[index, 'geometry'] = line\n",
    "    print(index+1,'files parsed.')    \n",
    "    \n",
    "\n",
    "geo_dataframe.to_file(r'C:\\garrett_workspace\\tableau\\strava_dashboard\\strava_dashboard.shp')\n",
    "\n",
    "crs_to_write = \"\"\"GEOGCS[\"GCS_WGS_1984\",DATUM[\"D_WGS_1984\",SPHEROID[\"WGS_1984\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.017453292519943295]]\"\"\"\n",
    "\n",
    "with open(r'C:\\garrett_workspace\\tableau\\strava_dashboard\\{}.prj'.format('strava_dashboard'), 'w') as file:\n",
    "    file.write(crs_to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hleqUr4CIQgC"
   },
   "source": [
    "## Cleaning up the Data\n",
    "\n",
    "The data provided by Strava can use a bit of tidying. Needed transforms include converting units to imperial, time fields to hours, dealing with some null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSjp5f6VJVKH"
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "data = gpd.read_file(r'C:\\garrett_workspace\\tableau\\strava_dashboard\\strava_dashboard.shp')\n",
    "\n",
    "data = data[['Activity I', 'Activity D', 'Activity N', 'Activity T', 'Elapsed Ti', \n",
    "             'Distance', 'Moving Tim', 'Average Sp',\n",
    "             'Elevation', 'Elevatio_1', 'Elevatio_2', 'Elevatio_3', 'Calories', 'geometry']]\n",
    "\n",
    "data.columns = ['id', 'date', 'name', 'type', 'duration', 'distance', 'moving_time', \n",
    "                'avg_speed', 'elev_gain', 'elev_1', 'elev_2', 'elev_3', 'calories', 'geometry']\n",
    "\n",
    "# shapefiles need datetime's as strings\n",
    "data['date'] = (pd.to_datetime(data['date'])).astype('str')\n",
    "# seconds to decimal hours\n",
    "data['duration'] = data['duration'] / 60 / 60\n",
    "# kms to miles\n",
    "data['distance'] = data['distance'] * 0.6213\n",
    "# seconds to decimal hours\n",
    "data['moving_time'] = data['moving_time'] / 60 / 60\n",
    "# I can't figure out what units are in this field so I recalculated avg_speed\n",
    "data['avg_speed'] = data['distance'] / data['moving_time']\n",
    "# using my personal knowledge of these activities, I was able to figure out how\n",
    "# to use these elev fields\n",
    "data['elev_gain'] = data['elev_gain'].fillna(data['elev_3'] - data['elev_2']).fillna(0)\n",
    "\n",
    "data = data.drop(['elev_1', 'elev_2', 'elev_3'], axis=1)\n",
    "# meters to feet\n",
    "data['elev_gain'] = (data['elev_gain'] * 3.28).astype('int')\n",
    "\n",
    "filename = 'activities_data'\n",
    "\n",
    "data.to_file(r'C:\\garrett_workspace\\tableau\\strava_dashboard\\{}.shp'.format(filename))\n",
    "# could rename the old projection file but this is just as easy\n",
    "crs_to_write = \"\"\"GEOGCS[\"GCS_WGS_1984\",DATUM[\"D_WGS_1984\",SPHEROID[\"WGS_1984\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.017453292519943295]]\"\"\"\n",
    "\n",
    "with open(r'C:\\garrett_workspace\\tableau\\strava_dashboard\\{}.prj'.format(filename), 'w') as file:\n",
    "    file.write(crs_to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_2Tlw1IKihF"
   },
   "source": [
    "## Load Data into Tabeau\n",
    "Far too easy!<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://github.com/bimewok/strava_tableau/blob/main/add_spatial_file.png?raw=true\" width=\"318\" height=\"313\" align=\"center\"/>\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWkWQWjAMMo9"
   },
   "source": [
    "## Build a Dashboard\n",
    "\n",
    "The rest of the data manipulation can easily be done within Tableau. My activities contain millions of coordinate points which can slow down the queries in the final dashboard. To fix this, you can resample the points and remove x out of every z points to reduce the size. As long as the resampling rate isn't too large, the end user wouldn't notice much detail loss in the map. Below is a simple way to do that in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKCqLCIMM-Ti"
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "data = gpd.read_file(r'C:\\garrett_workspace\\tableau\\strava_dashboard\\activities_data.shp')\n",
    "\n",
    "resample_factor = 4\n",
    "\n",
    "for row in range(len(data)):\n",
    "    '''the try - except block is because I added dummy null rows to represent weeks\n",
    "    that I had no activities, thus these rows have no geometry...'''\n",
    "    try:\n",
    "        linestring = data['geometry'][row]\n",
    "        simplified_points = []\n",
    "        for i in range(len(linestring.coords)):\n",
    "            coord = linestring.coords[i]\n",
    "            if i % resample_factor == 0:\n",
    "                simplified_points.append(coord)\n",
    "        data['geometry'][row].coords = simplified_points\n",
    "    except: \n",
    "        pass\n",
    "    \n",
    "data.to_file(r'C:\\garrett_workspace\\tableau\\strava_dashboard\\{}.shp'.format('activities_data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1i_Lz9tJNdOE"
   },
   "source": [
    "# The Dashboard!\n",
    "\n",
    "The dashboard can be found on Tableau Public here:\n",
    "https://public.tableau.com/profile/ben.garrett#!/vizhome/StravaVisualization/MyStravaData\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNCgAMeAcU0HYzdLJQCUzzP",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Strava Tableau.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
